<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>ada's blog</title>
    <link>https://chronovore.dev/posts</link>
    <atom:link rel="self" href="https://chronovore.dev/posts/feed.rss"/>
    <description>just a collection of thoughts~</description>
    <lastBuildDate>Thu, 17 Oct 2024 13:07:53 +0000</lastBuildDate>
    <generator>pumpkin</generator>
    <language>en</language>
    <item>
      <title>some notes about gaming on linux</title>
      <link>https://chronovore.dev/posts/2024-06-22-0436A-linux-gaming-notes.html</link>
      <pubDate>Sat, 22 Jun 2024 03:36:00 +0000</pubDate>
      <description>this is mostly so i don't forget</description>
      <author>Ada</author>
      <guid>tag:chronovore.dev,posts:2024-06-22-0436A-linux-gaming-notes</guid>
      <atom:content type="text/markdown"><![CDATA[I recently migrated to using Linux full time.
You need to not look far to find an ocean of reasons why Windows has been a bit miserable.

This post mainly serves as a logbook for fixes and workarounds for making games (and some applications) work on Linux.

[TOC]

## EAC "Failed to Intialize Dependencies" error

This is likely due to a `SDL_VIDEODRIVER` and/or `SDL_VIDEO_DRIVER` environment variable being present.

Try using the following launch arguments:

`env --unset=SDL_VIDEODRIVER --unset=SDL_VIDEO_DRIVER %command%`

Alternatively, set the variable to an empty string in whatever launch manager you're using (Heroic, Lutris).

## Enabling DX12 Ray Tracing

If you have a ray tracing capable GPU (RTX 2000 or newer, RX 6800 or newer)
you might be able to tell Mesa and VKD3D that ray-tracing can be enabled by using the following launch arguments:

`env RADV_PERFTEST=rt VKD3D_CONFIG=dxr VKD3D_FEATURE_LEVEL=12_2 %command%`

If you have an older GPU you can try:

`env RADV_PERFTEST=emulate_rt VKD3D_CONFIG=dxr VKD3D_FEATURE_LEVEL=12_2 %command%`

This assumes you have a relatively recent VKD3D and Mesa installation,
and the game has to support ray tracing in any capacity (i.e. World of Warcraft, Ratchet and Clank: Rift Apart)

## Games not capturing mouse cursor

Some games don't play nice with mouse capture, an easy way to solve this is by changing the launch arguments to:

`gamescope --force-grab-cursor -f %command%`

You may swap -f with -b for borderless windowed instead of fullscreen.

## Gamescope exiting early due to short-lived launcher processes

Some games launch via third party launchers that cause gamescope to exit before the wine device.

This may be solved by forcing the SDL backend.

`gamescope --backend sdl %command%`

## Gamescope resolution being fixed to the first window size under SDL

Gamescope under SDL has a hard time adjusting to resolution changes (i.e. if you're using it for launcher processes.), to solve this you must force the window to be fullscreen.

`gamescope -w YOUR_RESOLUTION_HERE -h YOUR_RESOLUTION_HERE -r YOUR_REFRESH_RATE_HERE --backend sdl --force-windows-fullscreen -f %command%`

(Special thanks to [Hollyrious](https://twitch.tv/hollyrious) for helping me diagnose this!)

## Windows-only Third-Party Mod Tooling

You can add the mod tools as a non-steam game, given it has a GUI.

When you do, force it to use **the same compatability tools as the game** and set the launch arguments to:

`env STEAM_COMPAT_DATA_PATH="~/.steam/root/steamapps/compatdata/489830" %command%`

You may need to install more dependencies like .NET 4.8/6.0, etc via protontricks, select the game *not* the non-steam app when installing.

Change `~/.steam/root/steamapps` to the SteamLibrary steamapps folder if installed in a steam library folder.

Change 489830 to match the steam id of the mod tool the game is for, 489830 is Skyrim SE/AE.
It is the number after `/app/` in the Steam store link, alternatively look it up on SteamDB.

## Game-Specific Fixes

### Dauntless

If you randomly crash, it might be because of stack smashing due to a race condition.

I have managed to reduce the frequency of crashes by limiting how many resources the game can use.

`env DXVK_CONFIG="dxvk.numCompilerThreads=1" WINE_CPU_TOPOLOGY="4:0,2,4,6" %command%`

If there is too much lag, you might want to redefine the CPU topology, i.e. `WINE_CPU_TOPOLOGY="8:0,1,2,3,4,5,6,7"`

## Changelog

*Update: 2024-09-22 - Added gamescope-related workarounds.*]]></atom:content>
    </item>
    <item>
      <title>of records and spans</title>
      <link>https://chronovore.dev/posts/2024-03-01-0720P-of-records-and-spans.html</link>
      <pubDate>Fri, 01 Mar 2024 19:20:00 +0000</pubDate>
      <description>this is just me rambling about a csharp feature</description>
      <author>Ada</author>
      <guid>tag:chronovore.dev,posts:2024-03-01-0720P-of-records-and-spans</guid>
      <atom:content type="text/markdown"><![CDATA[This post is mostly just me speaking fondly of Spans and Records
with no real technical information besides how cool they are and their use cases.

This is not a guide, or tutorial, or a technical writeup.
Just good vibes about new language features.

Beware! It does get a bit rambly at times.

Anyway, let's go back a few years, to the year 2015...

## Spans

.NET 2015 just released, bringing along with it better control over the
.NET Garbage Collector and a little footnote that reads:

> .NET Core packages such as the immutable collections, SIMD APIs, and
> networking APIs such as those found in the System.Net.Http namespace
> are now available as open-source packages on GitHub.

.NET was moving to open source.

Fast forward another 3 years, it's now 2018.

An update to .NET Core is released, .NET Core 2.1, bringing with it four new
types that represent a whole new programming paradigm. Span, and Memory.

Span is unique in that it's a **reference struct.** Preventing it from moving
out of the stack.

### Wait, C# has a stack?

Yes! C# has several tiers of memory management, the stack exists in the smallest
area. This space always exist, and it's where variables end up.
Every time you do a `new AwesomeClass()` or `new string[]`, it makes this in
the heap and just stores a bit of info on where this is in the heap in the stack

### So how does this relate to Span?

When you create a span via `new Span<T>()` or read a memory segment via `(Span<T>) SomeArray`,
this value entirely lives in the stack and only in the stack. Meaning you cannot
store it in the heap.

### Isn't this a massive issue with OOP designs?

Not if you consider what Span is used for. Span allows you to quickly wrap and
manipulate structures and arrays without actually copying memory repeatedly.

Let's say I'm reading a small file, and I need to read the opening for a blog
summary. This line is expected to be about... 40 characters long. What I can do
is read this line without actually allocating any heap memory using `stackalloc`
Turns out this is a massive speed improvement. Who guessed not allocating memory
is faster?

Memory is the functional equivalent of Span, except that it lives in the heap.
This means it has the luxury of granting a Span (without allocating more
memory!) and having access to all of the features, such as _Slicing._

### Slicing?

Slicing is taking a portion of an array, in such that you can manipulate and
move it around better. With memory and spans, it just makes a new struct with
the same data with some info to only work on that specific range.No data gets
copied, whereas making a slice in Arrays requires building (and copying!) a
whole second array.

Using Span (when allocated using `stackalloc`) and Memory buffers (if you need
to allocate in to the heap), you can easily avoid repeatedly allocating and
duplicating data which in turn results in major speed gains.

### But what about Native Interopability?

You know how, if you want to pass an array into a native library method,
you would have to do something like:

```cs
[DllImport] void NativeCall(byte* buffer, int size);

byte[] myArray = new byte[100];
unsafe {
    fixed(byte* pointer = &myArray[0]) {
        NativeCall(pointer, myArray.Length);
    }
}
```

Well, using Memory this is not only safer, but also a way nicer API.

```cs
Memory<byte> myArray = new byte[100];
using var memoryHandle = myArray.Pin();
NativeCall(memoryHandle.Pointer, myArray.Length);
```

Pretty cool huh?

## Records

We're still in 2018, so let's move forwards a bit to 2020.
.NET 5 just released, marking the start of a new era.
Dotnet is finally, fully and properly cross platform.
With it comes a new kind of structure, `record` and `record struct`

### How is this different from `class` and `struct`, aren't they just the same?

Functionally, there are a few different expectations with records that are
quite important. _Records prefer to be immutable._
It also introduced a few concepts such as
**primary constructors and init-only properties**

Let's imagine I have a class called `MeowAction` that is something like:

```cs
public class MeowAction {
    public float Volume { get; set; }
    public Subject MeowedAt { get; set; }
    public DateTimeOffset MeowedAtTime { get; set; } = DateTimeOffset.UtcNow;
   
    public MeowAction(float vol, Subject subject) {
        Volume = vol;
        MeowedAt = subject;
    }
}

// later...

new MeowAction(Volume.Loud, RandomStranger());
```

Simple meow-keeping class for a meow auditing system. However, as a data holding class
it's still doing some amount of predictable copying (which can be an issue!)

What if I told you this is the exact use case for Records?

Let's reimplement it as a record:

```cs
public record MeowAction(float Volume, Subject MeowedAt) {
    public DateTimeOffset MeowedAtTime { get; init; } = DateTimeOffset.UtcNow;
}
```

Way smaller, right?

### Wait, init? What happened to set?

With primary constructors in records, the default behavior is to mark properties as init-only.
As such, if you want to have the parameters as a normal get, set pair you would have to
something like:

```cs
public record MeowAction(float Volume, Subject MeowedAt) {
    public float Volume { get; set; } = Volume;
    public DateTimeOffset MeowedAtTime { get; init; } = DateTimeOffset.UtcNow;
}
```

### Could this not be integrated into a class?

As it turns out, it has.
There's nothing stopping you from using `class` instead of `record`

So why all this fanfare then? Because that's the beauty of records.
It shows that C# is willing to adopt new designs, such as...

### It has deconstructors for the primary constructor arguments

```cs
var action = new MeowAction(Volume.Loud, RandomStranger());
var (volume, subject) = action;
```

### It generates ToString

```cs
var action = new MeowAction(Volume.Loud, RandomStranger());
action.ToString();
// MeowAction { Volume = 85.0, MeowedAt = Person { Name = "Ada" }, MeowedAtTime = 2165-06-02 }
```

### It generates GetHashCode

```cs
var action = new MeowAction(Volume.Loud, RandomStranger());
action.GetHashCode(); // HashCode.Combine(Volume, MeowedAt, MeowedAtTime);
```

### It clones

```cs
var action = new MeowAction(Volume.Loud, RandomStranger());
var mutated = action with { Volume = Volume.VeryLoud };
```

### Cloning?!

Remember when I mentioned that Records prefer to be immutable?
This is how you get around that.

Any property marked with `set` or `init` can be mutated using curly braces like this.
The `with` keyword just happens to copy every value that isn't specified.

## Conclusion

This was a quite long but non-exhaustive fanfare for Span, Memory, and Records.
It's an exciting time to develop in C#, the language progressively is introducing
new featuers (such as nullability checks!) that improve code quality, speed and
overall encourage a more reliable way of programming.

I encourage you to consider weaving stackalloc, Span, Memory, Records,
and Struct Records into your projects targeting .NET 7 and newer.

Hint: A good use case of the `record` keyword are options or settings objects.

### Secret afterword

Did you know that you can have a `readonly record struct` which enforces complete immutability?

### Special Thanks

Thanks to [scarletquasar](https://github.com/scarletquasar) for some corrections.]]></atom:content>
    </item>
    <item>
      <title>json-ld is misused</title>
      <link>https://chronovore.dev/posts/2023-07-29-0925A-activity-jsonld-rot.html</link>
      <pubDate>Fri, 28 Jul 2023 08:25:00 +0000</pubDate>
      <description>it's not just fancy json.</description>
      <author>Ada</author>
      <guid>tag:chronovore.dev,posts:2023-07-29-0925A-activity-jsonld-rot</guid>
      <atom:content type="text/markdown"><![CDATA[# as always, context is important

ActivityStreams is a subset of JSON-LD, specifically the compacted form.

> This specification describes a JSON-based RFC7159 serialization syntax for the Activity Vocabulary that conforms to a subset of JSON-LD syntax constraints but does not require JSON-LD processing. While other serialization forms are possible, such alternatives are not discussed by this document[^assyntax].

> The serialized JSON form of an Activity Streams 2.0 document MUST be consistent with what would be produced by the standard JSON-LD 1.0 Processing Algorithms and JSON-LD-API Compaction Algorithm using, at least, the normative JSON-LD @context definition provided here[^asjsonld].

[^assyntax]: [https://www.w3.org/TR/activitystreams-core/#syntaxconventions](https://www.w3.org/TR/activitystreams-core/#syntaxconventions)
[^asjsonld]: [https://www.w3.org/TR/activitystreams-core/#jsonld](https://www.w3.org/TR/activitystreams-core/#jsonld0)

This essentially looks like fancy JSON with a schema attached to it.
But it's not... That `@context` is important, and **not** set in stone.

> For extensions, JSON-LD is used as the primary mechanism for defining and disambiguating extensions. Implementations that wish to fully support extensions SHOULD use JSON-LD mechanisms. 

> It is important to note that the JSON-LD Processing Algorithms, as currently defined, will silently ignore any property not defined in a JSON-LD @context. Implementations that publish Activity Streams 2.0 documents containing extension properties SHOULD provide a @context definition for all extensions[^asextensibility]. 

[^asextensibility]:[https://www.w3.org/TR/activitystreams-core/#extensibility](https://www.w3.org/TR/activitystreams-core/#extensibility)

While Activity vocabulary is set in stone and should not and will not ever change,
Extensions are not.

When sending an ActivityStreams message, you cannot rename, override or replace ActivityStreams predicates but this does not apply to extensions.

Given the following example:

```json
{
  "@context": [
    "https://www.w3.org/ns/activitystreams",
    {
        "foo": "http://example.org/foo"
    }
  ],
  "@id": "https://example.org/example/note",
  "type": "Note",
  "content": "This is a simple note",
  "foo": 123
}
```

This is still a valid ActivityStreams message, but **you should avoid accessing `"foo"` directly.**

Why? Overlap and disambiguation.

```json
{
  "@context": [
    "https://www.w3.org/ns/activitystreams",
    {
        "foo": "http://example.org/foo",
        "other_foo": "http://another.example.org/foo"
    }
  ],
  "@id": "https://example.org/example/note",
  "type": "Note",
  "content": "This is a simple note",
  "foo": 123,
  "other_foo": 456
}
```

Both `"foo"` and `"other_foo"` are `foo` terms. 
They can swap, or even be a different term entierly. It's implementation-defined.
This would still be a valid JSON-LD object and a valid Activity Object.

Then, there's also the use of IRIs.

```json
{   
  "@context": [
    "https://www.w3.org/ns/activitystreams",
    {
        "ex": "http://example.org/",
        "foo": "ex:foo",
        "ex2": "http://another.example.org/"
    }
  ],
  "@id": "https://example.org/example/note",
  "type": "Note",
  "content": "This is a simple note",
  "foo": 123,
  "ex2:foo": 456
}
```

This one is more set-in-stone, however you should still verify that the namespace is actually what you expect. 
However now everyone is forced to use "ex2:foo" if we were to include this extension (much like we're currently forced to use the short IRI `"vcard:location"`)

JSON-LD algorithms would transform all three objects, essentially into:

```json
{
  "@id": "https://example.org/example/note",
  "https://www.w3.org/ns/activitystreams/type": "Note",
  "https://www.w3.org/ns/activitystreams/content": "This is a simple note",
  "http://example.org/foo": 123,
  "http://another.example.org/foo": 456
}
```

and

```json
{
  "@id": "https://example.org/example/note",
  "as:type": "Note",
  "as:content": "This is a simple note",
  "ex:foo": 123,
  "ex2:foo": 456
}
```

You would then just access it via either it's shortened IRI, or it's full URI-- disambiguating the result.
This is very important because as ActivityPub is getting more popular, and more third-party extensions are introduced by Litepub, Misskey, and others.

I found this out when I was looking into adding `schema:license` and `schema:description` into ActivityStreams' `Image` type. 
To provide a means to define a copyright SPDX and/or author citation, while also allowing for descriptive text[^ap-descriptive-text].
As it stands right now, i'm using `schema:license` and `schema:description` directly into the object;
now knowing that every implementation will have to blindly check that short IRI.

[^ap-descriptive-text]: ActivityPub nor ActivityStreams defines how descriptive text is supposed to be federated. Right now most implementations store in the `"name"` field of the Image which I find a bit silly especially since new implementations might be unaware of this quirk and potentially use a giant blob of text as the filename if they make the same assumption I did.

The only social-network-style ActivityPub implementation that I've found that implements proper JSON-LD parsing is GoToSocial,
which very effectively utilizes Go's struct tags to map JSON-LD URIs to struct fields.

I do understand that processing JSON-LD at all is way more computationally heavy than JSON by itself, 
but if you're going to introduce JSON-LD extension contexts **please** create a json-ld spec file[^its-just-xml].

You can federate anything over ActivityStreams, even new Activity types if you wanted-- as long as you properly define them.
Support for them in other platforms might not ever exist, though.

[^its-just-xml]: Compacted JSON-LD is essentially just JSON with XMLNS DTD features.]]></atom:content>
    </item>
    <item>
      <title>compression algorithms</title>
      <link>https://chronovore.dev/posts/2023-01-25-1234P-compression-deepdive.html</link>
      <pubDate>Wed, 25 Jan 2023 14:42:00 +0000</pubDate>
      <description>a deep dive into compression algorithms and how to notice them in hex</description>
      <author>Ada</author>
      <guid>tag:chronovore.dev,posts:2023-01-25-1234P-compression-deepdive</guid>
      <atom:content type="text/markdown"><![CDATA[# Compression Algorithms

One of the things that my programmer friends often ask me about is how I can tell what kind of compression algorithm is used by a file.
This is an interesting question, and I hope that this post will help you understand how I notice compression algorithms in hex.

I will not be going over the fundamentals of compression algorithms or go into detail about how they work.

The forum posts[^eyes] and reference docs that have taught me how to do this are referenced where appropriate.

I will explain some of the structure of how the compression algorithms are set up because I believe that understanding _what_ these values mean,
it will help you understand _why_ they are there and how to notice them when the configuration values are anything but the defaults.

All magic values are written as byte sequences (i.e. big endian)

[^eyes]: [https://zenhax.com/viewtopic.php?t=27](https://web.archive.org/web/20230109220055/https://zenhax.com/viewtopic.php?t=27) (archived)

[TOC]

## The Basics

The first thing you need to know is that compression algorithms are not magic.
In many cases compression algorithms have a sanity check (a "magic" number) that is used to verify and set up the decompressor.
In other cases parts of the data can be seen in the compressed data.

## The Dreaded Lempel-Ziv Algorithm (Lz*)

The Lempel-Ziv algorithm is a compression algorithm that is used in many compression formats.
It comes in a lot of flavors and figuring out which one is used can be difficult.

To figure out if a file might be compressed with an Lz algorithm, you should look for the following:

- The first byte is almost always `0F`, `1F`, `F0`, or `FF`
- The following bytes are seemingly uncompressed data.
- The first byte repeats itself frequently in the data, especially at the start of the file.

This is because LZ algorithms use a dictionary to store data that has been seen before,
and the first byte (the "block") is used to determine the length of the data to be copied from the dictionary.

I strongly suggest using comscan[^comscan] with quickbms[^bms] to test what compression algorithm is used by a file when you encounter this and LZ4 (see below) does not work.

[^comscan]: [https://zenhax.com/viewtopic.php?t=23](https://web.archive.org/web/20221125023314/https://zenhax.com/viewtopic.php?t=23) (archived)

[^bms]: [https://aluigi.altervista.org/quickbms.htm](https://aluigi.altervista.org/quickbms.htm)

### LZ4

LZ4[^lz4] is a common compression algorithm used especially in video games during the 2010s.

LZ4's block has the following format:

```c
struct lz4_block {
    uint8_t encode_count : 4;
    uint8_t literal_count : 4;
}
```

The `encode_count` is the number of bytes to copy from the decompressed stream, and the `literal_count` is the number of bytes to copy from the compressed data.
There is also a special case where either byte is `0F`, which means that the next bytes are added to the count (until the byte is no longer `FF`).
The minimal number of literals read is 4.

[^lz4]: [https://github.com/lz4/lz4/](https://github.com/lz4/lz4/)


### LZMA, and LZMA2

LZMA[^7z] has no hard defined header[^lzma], though it will often start with `5D` or `2C` followed by a 32-bit integer that is the size of the inline dictionary data (usually zero.)

LZMA2 likewise has no header, though it will often start with `18` followed by compressed data. Note that this byte is optional.

I have not yet seen a raw LZMA stream in the wild beyond 7z files, likely due to it's large overhead.

[^7z]: [https://7-zip.org/sdk.html](https://7-zip.org/sdk.html)
[^lzma]: [https://github.com/jljusten/LZMA-SDK/blob/20d713a28e5aee284f5671c7cf41ffa52db0215e/DOC/lzma-specification.txt](https://github.com/jljusten/LZMA-SDK/blob/20d713a28e5aee284f5671c7cf41ffa52db0215e/DOC/lzma-specification.txt)


## DEFLATE, Zlib, and GZip

DEFLATE[^zlib] is a compression algorithm that is used in many files, and you will most likely have already seen it if you do any amount of file analysis.

ZLib uses a DEFLATE block with a header, and ADLER32 as it's checksum algorithm.

[^zlib]: [https://github.com/madler/zlib](https://github.com/madler/zlib)

### ZLib

ZLib[^rfc1950] preprends a 2 byte header to the compressed block (usually deflate). This usually is `78 9C` or `78 DA`

The first byte is the compression method, and the second byte has some flags. The compression method is usually `8`, which is DEFLATE.

```c
struct zlib_header {
    uint8_t compression_method : 4;
    uint8_t compression_info : 4;
    uint8_t checksum : 5;
    uint8_t dict : 1;
    uint8_t level : 2;
}
```

The `compression_info` is the log base 2 of the window size (the size of the dictionary used by the compressor), and the `checksum` a the checksum of the header.
The `dict` flag is set if a dictionary is used, and the `level` is the compression level used by the compressor.

Knowing this, zlib header will always start with `78` if the compression method is DEFLATE.

[^rfc1950]: [https://tools.ietf.org/html/rfc1950](https://tools.ietf.org/html/rfc1950)

### DEFLATE

A "raw" DEFLATE[^rfc1951] stream is not very common, but it is still used in some places (especially in files produced by C# projects).

It usually starts with `C#` or `E#` but it's a bit more complicated than that.

The DEFLATE block has the following format:

```c
struct deflate_block {
    uint8_t final : 1;
    uint8_t type : 2;
}
```

The `final` flag is set if this is the last block in the stream, and the `type` is the type of block.

[^rfc1951]: [https://tools.ietf.org/html/rfc1951](https://tools.ietf.org/html/rfc1951)

### GZip

GZip[^rfc1952] is a ZLib stream with a well formed header.

GZip will always start with a magic number (`1F 8B`) as well as a compression method (`8` for DEFLATE).

The header has the following format:

```c
struct gzip_header {
    uint16_t magic;
    uint8_t compression_method;
    uint8_t flags;
    uint32_t timestamp;
    uint8_t xtra_flags;
    uint8_t os;
}
```

[^rfc1952]: [https://tools.ietf.org/html/rfc1952](https://tools.ietf.org/html/rfc1952)

## ZStandard

ZStandard[^zstd] (zstd) is a relatively new compression algorithm that is starting to be used in many places due to it's ability to have very high compression ratios with specialized dictionaries.

Fortunately, ZStandard has a magic number that is used to identify the file, this usually is `## B5 2F FD` with the unknown byte being the specific version.

The ZStandard header has the following format[^zstddoc]:

```c
struct zstd_header {
    uint32_t magic;
    uint8_t content_size_flag : 2;
    uint8_t single_segment_flag : 1;
    uint8_t unused : 1;
    uint8_t reserved : 1;
    uint8_t checksum_flag : 1;
    uint8_t dict_id_flag : 2;
}
```

[^zstd]: [https://github.com/facebook/zstd](https://github.com/facebook/zstd)
[^zstddoc]: [https://github.com/facebook/zstd/blob/3732a08f5b82ed87a744e65daa2f11f77dabe954/doc/zstd_compression_format.md](https://github.com/facebook/zstd/blob/3732a08f5b82ed87a744e65daa2f11f77dabe954/doc/zstd_compression_format.md)

### ZDictionary

ZStandard might use a dictionary[^zdict] to compress the data, and the dictionary is stored either as a separate file, or in the same file as the compressed data. In some cases it might be in the executable itself (very rare!)
Documentation on ZDict is sparse, however we know that the magic value is `37 A4 30 EC`

The ZDict header has the following format:

```c
struct zdict_header {
    uint32_t magic;
    uint32_t dict_id;
}
```

[^zdict]: [https://github.com/facebook/zstd/blob/3732a08f5b82ed87a744e65daa2f11f77dabe954/doc/zstd_compression_format.md#dictionary-format](https://github.com/facebook/zstd/blob/3732a08f5b82ed87a744e65daa2f11f77dabe954/doc/zstd_compression_format.md#dictionary-format)

## Oodle

Oodle[^oodle] is a proprietary compression format used in many games, and has a hardware encoder in the PS5.

Oodle will always start with `#C` if it is made with version 4 or higher. Version 4 Oodle files have the following format:

```c
struct oodle_block_header {
    uint8_t magic : 4;
    uint8_t version : 2;
    bool copy : 1;
    bool reset : 1;
    uint8_t compression_type : 7;
    bool has_checksum : 1;
}
```

Compression type will be between 0 and 13 as of Oodle Version 9 (oo2core_9), and the checksum used is a modified Jenkins algorithm.
From this we can deduce that the second byte will be between `00` and `0D`, or `80` and `8D`

Note that Oodle will still load version 3 and older files, which will start with `#0`, `#1`, `#2`, or `#3` and will  usually look like LZW or LZB.

[^oodle]: [http://www.radgametools.com/oodle.htm](http://www.radgametools.com/oodle.htm)

## Tile Streaming (dstorage)

Tile Streaming uses a system to decompress multiple blocks simultaneously[^dstorage].
The format supports various compression formats, but at the moment only GDeflate[^gdeflate] (a variation of DEFLATE) is recognized.
The header is easily tested, the first byte will be the compression type (`04` for GDeflate) followed by that byte XORed with 0xFF (`FB` for GDeflate.) Followed by the number of "tiles".

Data compressed with GDeflate Tile Streaming will start with `04 FB`.

This ends up being:

```c
struct tile_stream_header {
    uint8_t compressor_id;
    uint8_t magic;
    uint16_t num_tiles;
    uint32_t tile_size_idx : 2; // this is always 1
    uint32_t last_tile_size: 18;
    uint32_t reserved : 12;
};
```

[^dstorage]: [https://github.com/microsoft/DirectStorage/tree/56489d25900d916a9cc450f5efe9e62b01789030/GDeflate/GDeflate](https://github.com/microsoft/DirectStorage/tree/56489d25900d916a9cc450f5efe9e62b01789030/GDeflate/GDeflate)
[^gdeflate]: [https://github.com/NVIDIA/libdeflate](https://github.com/NVIDIA/libdeflate)

## DENSITY

Density[^density] is a compression algorithm that claims[^density-benchmarks] 2x decompression speed compared to LZ4.

It has a very predictable header that is easy to spot.

```c
struct density_header {
    uint8_t version_major;
    uint8_t version_minor;
    uint8_t version_revision;
    uint8_t compression_type;
    uint32_t reserved;
}
```

Density has 18 releases, of which only 3 are not marked as pre released (0.14.0, 0.14.1, 0.14.2). It has 3 compression types (1, 2, 3.)

We can reduce this to a set byte sequences `00 0E 02 01 00 00 00 00` with 02 being the revision version, and 01 being the compression type.

[^density]: [https://github.com/g1mv/density](https://github.com/g1mv/density)
[^density-benchmarks]: [https://github.com/g1mv/density?tab=readme-ov-file#benchmarks](https://github.com/g1mv/density?tab=readme-ov-file#benchmarks)

## Zip Signature Speedrun

Compression archives almost always have a signature at the start of the file.
I'm adding them here for completeness.

- ZIP Magic: `50 4B` (PK)
- BZip2 Magic: `42 5A 68` (BZh)
- 7Zip Magic: `37 7A BC AF 27 1C` (7z)
- Rar Magic: `52 61 72 21 1A 07` (Rar!)
- WIM Magic: `4D 53 57 49 4D 00 00 00` (MSWIM)
- Xz Magic: `FD 37 7A 58 5A 00` (7zXZ)
- Tar Magic: `75 73 74 61 72` (ustar) - usually found at the end of filelist

## Changelog

*Update: 2024-09-09 - Added DENSITY info.*

*Update: 2024-08-19 - Added Tile Streaming info.*

*Update: 2023-07-11 - zenhax is offline, replaced links with archive.org links.*]]></atom:content>
    </item>
  </channel>
</rss>
